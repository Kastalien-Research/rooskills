{
  "markdown": "Get comprehensive, actionable reviews directly in your PRs. [Try Roo's PR Reviewer](https://roocode.com/reviewer?utm_source=docs&utm_medium=banner&utm_campaign=reviewer_promo)\n\n[Skip to main content](https://docs.roocode.com/providers/ollama#__docusaurus_skipToContent_fallback)\n\n[![Roo Code Logo](https://docs.roocode.com/img/roo-code-logo-dark.png)](https://docs.roocode.com/)\n\n`ctrl`  `K`\n\n[Reddit](https://www.reddit.com/r/RooCode/ \"Reddit\")[Discord](https://discord.gg/roocode \"Discord\")\n\n[20.3k](https://github.com/RooCodeInc/Roo-Code \"GitHub Repository\") [Install ·934.6k](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline \"Install VS Code Extension\")\n\n- [Welcome](https://docs.roocode.com/)\n- [Getting Started](https://docs.roocode.com/providers/ollama#)\n\n- [Tutorial Videos](https://docs.roocode.com/providers/ollama#)\n\n- [Roo Code Cloud](https://docs.roocode.com/providers/ollama#)\n\n- Features\n\n  - [API Configuration Profiles](https://docs.roocode.com/features/api-configuration-profiles)\n  - [Auto-Approving Actions](https://docs.roocode.com/features/auto-approving-actions)\n  - [Boomerang Tasks](https://docs.roocode.com/features/boomerang-tasks)\n  - [Browser Use](https://docs.roocode.com/features/browser-use)\n  - [Checkpoints](https://docs.roocode.com/features/checkpoints)\n  - [Code Actions](https://docs.roocode.com/features/code-actions)\n  - [Codebase Indexing](https://docs.roocode.com/features/codebase-indexing)\n  - [Custom Instructions](https://docs.roocode.com/features/custom-instructions)\n  - [Diagnostics Integration](https://docs.roocode.com/features/diagnostics-integration)\n  - [Customizing Modes](https://docs.roocode.com/features/custom-modes)\n  - [Diff/Fast Edits](https://docs.roocode.com/features/fast-edits)\n  - [Enhance Prompt](https://docs.roocode.com/features/enhance-prompt)\n  - [Import/Export/Reset Settings](https://docs.roocode.com/features/settings-management)\n  - [Intelligent Context Condensing](https://docs.roocode.com/features/intelligent-context-condensing)\n  - [Keyboard Navigation](https://docs.roocode.com/features/keyboard-shortcuts)\n  - [Roo Code Marketplace](https://docs.roocode.com/features/marketplace)\n  - [Message Queueing](https://docs.roocode.com/features/message-queueing)\n  - [Model Temperature](https://docs.roocode.com/features/model-temperature)\n  - [Multi-File Reads](https://docs.roocode.com/features/concurrent-file-reads)\n  - [.rooignore](https://docs.roocode.com/features/rooignore)\n  - [Slash Commands](https://docs.roocode.com/features/slash-commands)\n  - [Suggested Responses](https://docs.roocode.com/features/suggested-responses)\n  - [Task Todo List](https://docs.roocode.com/features/task-todo-list)\n  - [Terminal Shell Integration](https://docs.roocode.com/features/shell-integration)\n  - [MCP](https://docs.roocode.com/providers/ollama#)\n\n  - [Experimental](https://docs.roocode.com/providers/ollama#)\n\n  - [Additional Features](https://docs.roocode.com/features/more-features)\n- [Advanced Usage](https://docs.roocode.com/providers/ollama#)\n\n- [Model Providers](https://docs.roocode.com/providers/ollama#)\n\n  - [Anthropic](https://docs.roocode.com/providers/anthropic)\n  - [Claude Code](https://docs.roocode.com/providers/claude-code)\n  - [AWS Bedrock](https://docs.roocode.com/providers/bedrock)\n  - [Cerebras](https://docs.roocode.com/providers/cerebras)\n  - [DeepInfra](https://docs.roocode.com/providers/deepinfra)\n  - [DeepSeek](https://docs.roocode.com/providers/deepseek)\n  - [Doubao](https://docs.roocode.com/providers/doubao)\n  - [Featherless AI](https://docs.roocode.com/providers/featherless)\n  - [Fireworks AI](https://docs.roocode.com/providers/fireworks)\n  - [Chutes AI](https://docs.roocode.com/providers/chutes)\n  - [Google Gemini](https://docs.roocode.com/providers/gemini)\n  - [Glama](https://docs.roocode.com/providers/glama)\n  - [Groq](https://docs.roocode.com/providers/groq)\n  - [Hugging Face](https://docs.roocode.com/providers/huggingface)\n  - [Human Relay](https://docs.roocode.com/providers/human-relay)\n  - [IO Intelligence Provider](https://docs.roocode.com/providers/io-intelligence)\n  - [LM Studio](https://docs.roocode.com/providers/lmstudio)\n  - [LiteLLM](https://docs.roocode.com/providers/litellm)\n  - [Mistral AI](https://docs.roocode.com/providers/mistral)\n  - [Ollama](https://docs.roocode.com/providers/ollama)\n  - [OpenAI](https://docs.roocode.com/providers/openai)\n  - [OpenAI Compatible](https://docs.roocode.com/providers/openai-compatible)\n  - [OpenRouter](https://docs.roocode.com/providers/openrouter)\n  - [Qwen Code CLI](https://docs.roocode.com/providers/qwen-code)\n  - [Requesty](https://docs.roocode.com/providers/requesty)\n  - [Roo Code Cloud](https://docs.roocode.com/providers/roo-code-cloud)\n  - [SambaNova](https://docs.roocode.com/providers/sambanova)\n  - [Unbound](https://docs.roocode.com/providers/unbound)\n  - [Vercel AI Gateway](https://docs.roocode.com/providers/vercel-ai-gateway)\n  - [GCP Vertex AI](https://docs.roocode.com/providers/vertex)\n  - [VS Code Language Model API](https://docs.roocode.com/providers/vscode-lm)\n  - [xAI (Grok)](https://docs.roocode.com/providers/xai)\n  - [Z AI](https://docs.roocode.com/providers/zai)\n- [FAQ](https://docs.roocode.com/providers/ollama#)\n\n- [Contributing (GitHub)](https://github.com/RooCodeInc/Roo-Code/blob/main/CONTRIBUTING.md)\n- [Roocabulary (GitHub)](https://github.com/cannuri/Roocabulary)\n- [Update Notes](https://docs.roocode.com/providers/ollama#)\n\n\n- [Home page](https://docs.roocode.com/)\n- Model Providers\n- Ollama\n\nCopy Page\n\nOn this page\n\n# Using Ollama With Roo Code\n\nRoo Code supports running models locally using Ollama. This provides privacy, offline access, and potentially lower costs, but requires more setup and a powerful computer.\n\n**Website:** [https://ollama.com/](https://ollama.com/)\n\n* * *\n\n## Setting up Ollama [​](https://docs.roocode.com/providers/ollama\\#setting-up-ollama \"Direct link to Setting up Ollama\")\n\n1. **Download and Install Ollama:** Download the Ollama installer for your operating system from the [Ollama website](https://ollama.com/). Follow the installation instructions. Make sure Ollama is running\n\n\n\n\n\n```codeBlockLines_e6Vv\nollama serve\n\n```\n\n2. **Download a Model:** Ollama supports many different models. You can find a list of available models on the [Ollama website](https://ollama.com/library). Some recommended models for coding tasks include:\n\n\n   - `codellama:7b-code` (good starting point, smaller)\n   - `codellama:13b-code` (better quality, larger)\n   - `codellama:34b-code` (even better quality, very large)\n   - `qwen2.5-coder:32b`\n   - `mistralai/Mistral-7B-Instruct-v0.1` (good general-purpose model)\n   - `deepseek-coder:6.7b-base` (good for coding tasks)\n   - `llama3:8b-instruct-q5_1` (good for general tasks)\n\nTo download a model, open your terminal and run:\n\n```codeBlockLines_e6Vv\nollama pull <model_name>\n\n```\n\nFor example:\n\n```codeBlockLines_e6Vv\nollama pull qwen2.5-coder:32b\n\n```\n\n3. **Configure the Model:** Configure your model's context window in Ollama and save a copy.\n\n\n\nDefault Context Behavior\n\n\n\n\n\n**Roo Code automatically defers to the Modelfile's `num_ctx` setting by default.** When you use a model with Ollama, Roo Code reads the model's configured context window and uses it automatically. You don't need to configure context size in Roo Code settings - it respects what's defined in your Ollama model.\n\n\n\n\n\n**Option A: Interactive Configuration**\n\nLoad the model (we will use `qwen2.5-coder:32b` as an example):\n\n\n\n\n\n```codeBlockLines_e6Vv\nollama run qwen2.5-coder:32b\n\n```\n\n\n\n\n\n\n\n\n\nChange context size parameter:\n\n\n\n\n\n```codeBlockLines_e6Vv\n/set parameter num_ctx 32768\n\n```\n\n\n\n\n\n\n\n\n\nSave the model with a new name:\n\n\n\n\n\n```codeBlockLines_e6Vv\n/save your_model_name\n\n```\n\n\n\n\n\n\n\n\n\n**Option B: Using a Modelfile (Recommended)**\n\nCreate a `Modelfile` with your desired configuration:\n\n\n\n\n\n```codeBlockLines_e6Vv\n# Example Modelfile for reduced context\nFROM qwen2.5-coder:32b\n\n# Set context window to 32K tokens (reduced from default)\nPARAMETER num_ctx 32768\n\n# Optional: Adjust temperature for more consistent output\nPARAMETER temperature 0.7\n\n# Optional: Set repeat penalty\nPARAMETER repeat_penalty 1.1\n\n```\n\n\n\n\n\n\n\n\n\nThen create your custom model:\n\n\n\n\n\n```codeBlockLines_e6Vv\nollama create qwen-32k -f Modelfile\n\n```\n\n\n\n\n\n\n\n\n\n\n\nOverride Context Window\n\n\n\n\n\nIf you need to override the model's default context window:\n\n\n\n- **Permanently:** Save a new model version with your desired `num_ctx` using either method above\n- **Roo Code behavior:** Roo automatically uses whatever `num_ctx` is configured in your Ollama model\n- **Memory considerations:** Reducing `num_ctx` helps prevent out-of-memory errors on limited hardware\n\n4. **Configure Roo Code:**\n   - Open the Roo Code sidebar ( icon).\n   - Click the settings gear icon ().\n   - Select \"ollama\" as the API Provider.\n   - Enter the model tag or saved name from the previous step (e.g., `your_model_name`).\n   - (Optional) Configure the base URL if you're running Ollama on a different machine. The default is `http://localhost:11434`.\n   - (Optional) Enter an API Key if your Ollama server requires authentication.\n   - (Advanced) Roo uses Ollama's native API by default for the \"ollama\" provider. An OpenAI-compatible `/v1` handler also exists but isn't required for typical setups.\n\n* * *\n\n## Tips and Notes [​](https://docs.roocode.com/providers/ollama\\#tips-and-notes \"Direct link to Tips and Notes\")\n\n- **Resource Requirements:** Running large language models locally can be resource-intensive. Make sure your computer meets the minimum requirements for the model you choose.\n- **Model Selection:** Experiment with different models to find the one that best suits your needs.\n- **Offline Use:** Once you've downloaded a model, you can use Roo Code offline with that model.\n- **Token Tracking:** Roo Code tracks token usage for models run via Ollama, helping you monitor consumption.\n- **Ollama Documentation:** Refer to the [Ollama documentation](https://ollama.com/docs) for more information on installing, configuring, and using Ollama.\n\n* * *\n\n## Troubleshooting [​](https://docs.roocode.com/providers/ollama\\#troubleshooting \"Direct link to Troubleshooting\")\n\n### Out of Memory (OOM) on First Request [​](https://docs.roocode.com/providers/ollama\\#out-of-memory-oom-on-first-request \"Direct link to Out of Memory (OOM) on First Request\")\n\n**Symptoms**\n\n- First request from Roo fails with an out-of-memory error\n- GPU/CPU memory usage spikes when the model first loads\n- Works after you manually start the model in Ollama\n\n**Cause**\nIf no model instance is running, Ollama spins one up on demand. During that cold start it may allocate a larger context window than expected. The larger context window increases memory usage and can exceed available VRAM or RAM. This is an Ollama startup behavior, not a Roo Code bug.\n\n**Fixes**\n\n1. **Preload the model**\n\n\n\n\n\n```codeBlockLines_e6Vv\nollama run &lt;model-name&gt;\n\n```\n\n\n\n\n\n\n\n\n\nKeep it running, then issue the request from Roo.\n\n2. **Pin the context window ( `num_ctx`)**\n   - Option A — interactive session, then save:\n\n\n\n\n     ```codeBlockLines_e6Vv\n     # inside `ollama run &lt;base-model&gt;`\n     /set parameter num_ctx 32768\n     /save &lt;your_model_name&gt;\n\n     ```\n\n   - Option B — Modelfile (recommended for reproducibility):\n\n\n\n\n     ```codeBlockLines_e6Vv\n     FROM &lt;base-model&gt;\n     PARAMETER num_ctx 32768\n     # Adjust based on your available memory:\n     # 16384 for ~8GB VRAM\n     # 32768 for ~16GB VRAM\n     # 65536 for ~24GB+ VRAM\n\n     ```\n\n\n\n\n\n\n\n\n     Then create the model:\n\n\n\n\n     ```codeBlockLines_e6Vv\n     ollama create &lt;your_model_name&gt; -f Modelfile\n\n     ```\n3. **Ensure the model's context window is pinned**\nSave your Ollama model with an appropriate `num_ctx` (via `/set` \\+ `/save`, or preferably a Modelfile). **Roo Code automatically detects and uses the model's configured `num_ctx`** \\- there is no manual context size setting in Roo Code for the Ollama provider.\n\n4. **Use smaller variants**\nIf GPU memory is limited, use a smaller quant (e.g., q4 instead of q5) or a smaller parameter size (e.g., 7B/13B instead of 32B).\n\n5. **Restart after an OOM**\n\n\n\n\n\n```codeBlockLines_e6Vv\nollama ps\nollama stop &lt;model-name&gt;\n\n```\n\n\n**Quick checklist**\n\n- Model is running before Roo request\n- `num_ctx` pinned (Modelfile or `/set` \\+ `/save`)\n- Model saved with appropriate `num_ctx` (Roo uses this automatically)\n- Model fits available VRAM/RAM\n- No leftover Ollama processes\n\n[Edit this page](https://github.com/RooCodeInc/Roo-Code-Docs/edit/main/docs/providers/ollama.md)\n\nLast updated on **Oct 3, 2025**\n\n[Previous\\\\\n\\\\\nMistral AI](https://docs.roocode.com/providers/mistral) [Next\\\\\n\\\\\nOpenAI](https://docs.roocode.com/providers/openai)\n\n- [Setting up Ollama](https://docs.roocode.com/providers/ollama#setting-up-ollama)\n- [Tips and Notes](https://docs.roocode.com/providers/ollama#tips-and-notes)\n- [Troubleshooting](https://docs.roocode.com/providers/ollama#troubleshooting)\n  - [Out of Memory (OOM) on First Request](https://docs.roocode.com/providers/ollama#out-of-memory-oom-on-first-request)\n\n![Roo Code Logo](https://docs.roocode.com/img/roo-code-logo-dark.png)\n\nEmpowering developers to build better software faster with AI-powered tools and insights.\n\n[GitHub](https://github.com/RooCodeInc/Roo-Code)[Discord](https://discord.gg/roocode)[Reddit](https://www.reddit.com/r/RooCode/)[X (Twitter)](https://x.com/roo_code)[LinkedIn](https://www.linkedin.com/company/roo-code)[TikTok](https://www.tiktok.com/@roo.code)[Bluesky](https://bsky.app/profile/roocode.bsky.social)\n\nGitHub\n\n- [Issues](https://github.com/RooCodeInc/Roo-Code/issues)\n- [Feature Requests](https://github.com/RooCodeInc/Roo-Code/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)\n\nDownload\n\n- [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline)\n- [Open VSX Registry](https://open-vsx.org/extension/RooVeterinaryInc/roo-cline)\n\nCompany\n\n- [Contact](mailto:support@roocode.com)\n- [Careers](https://careers.roocode.com/)\n- [Website Privacy Policy](https://roocode.com/privacy)\n- [Extension Privacy Policy](https://github.com/RooCodeInc/Roo-Code/blob/main/PRIVACY.md)\n\nLike most of the internet, we use cookies. Are you OK with that?\n\nDeclineAccept",
  "metadata": {
    "language": "en",
    "ogDescription": "Set up Ollama with Roo Code to run open source language models locally for privacy, offline access, and cost-effective AI coding.",
    "docsearch:docusaurus_tag": "docs-default-current",
    "twitter:site": "@roo_code",
    "generator": "Docusaurus v3.9.2",
    "ogTitle": "Using Ollama With Roo Code | Roo Code Documentation",
    "docusaurus_version": "current",
    "ogUrl": "https://docs.roocode.com/providers/ollama",
    "ogImage": "https://docs.roocode.com/img/social-share.jpg",
    "description": "Set up Ollama with Roo Code to run open source language models locally for privacy, offline access, and cost-effective AI coding.",
    "docsearch:language": "en",
    "og:description": "Set up Ollama with Roo Code to run open source language models locally for privacy, offline access, and cost-effective AI coding.",
    "docusaurus_tag": "docs-default-current",
    "twitter:creator": "@roo_code",
    "og:type": "website",
    "og:url": "https://docs.roocode.com/providers/ollama",
    "og:locale": "en_US",
    "twitter:card": "summary_large_image",
    "docsearch:version": "current",
    "favicon": "https://docs.roocode.com/img/favicon.ico",
    "viewport": "width=device-width, initial-scale=1.0",
    "og:image": "https://docs.roocode.com/img/social-share.jpg",
    "keywords": "Ollama,local models,Roo Code,open source AI,CodeLlama,Qwen,offline AI,privacy,context window configuration",
    "ogLocale": "en_US",
    "docusaurus_locale": "en",
    "twitter:image": "https://docs.roocode.com/img/social-share.jpg",
    "og:title": "Using Ollama With Roo Code | Roo Code Documentation",
    "title": "Using Ollama With Roo Code | Roo Code Documentation",
    "scrapeId": "8eb2b283-c5af-4671-9f1e-710ab551cf8a",
    "sourceURL": "https://docs.roocode.com/providers/ollama",
    "url": "https://docs.roocode.com/providers/ollama",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic",
    "cacheState": "miss",
    "indexId": "fc54c5d8-d4ed-4515-a957-a781ff54b4cc",
    "creditsUsed": 1
  }
}