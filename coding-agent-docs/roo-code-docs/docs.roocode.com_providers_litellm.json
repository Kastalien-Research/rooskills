{
  "markdown": "Get comprehensive, actionable reviews directly in your PRs. [Try Roo's PR Reviewer](https://roocode.com/reviewer?utm_source=docs&utm_medium=banner&utm_campaign=reviewer_promo)\n\n[Skip to main content](https://docs.roocode.com/providers/litellm#__docusaurus_skipToContent_fallback)\n\n[![Roo Code Logo](https://docs.roocode.com/img/roo-code-logo-dark.png)](https://docs.roocode.com/)\n\n`ctrl`  `K`\n\n[Reddit](https://www.reddit.com/r/RooCode/ \"Reddit\")[Discord](https://discord.gg/roocode \"Discord\")\n\n[20.3k](https://github.com/RooCodeInc/Roo-Code \"GitHub Repository\") [Install ·935.8k](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline \"Install VS Code Extension\")\n\n- [Welcome](https://docs.roocode.com/)\n- [Getting Started](https://docs.roocode.com/providers/litellm#)\n\n- [Tutorial Videos](https://docs.roocode.com/providers/litellm#)\n\n- [Roo Code Cloud](https://docs.roocode.com/providers/litellm#)\n\n- Features\n\n  - [API Configuration Profiles](https://docs.roocode.com/features/api-configuration-profiles)\n  - [Auto-Approving Actions](https://docs.roocode.com/features/auto-approving-actions)\n  - [Boomerang Tasks](https://docs.roocode.com/features/boomerang-tasks)\n  - [Browser Use](https://docs.roocode.com/features/browser-use)\n  - [Checkpoints](https://docs.roocode.com/features/checkpoints)\n  - [Code Actions](https://docs.roocode.com/features/code-actions)\n  - [Codebase Indexing](https://docs.roocode.com/features/codebase-indexing)\n  - [Custom Instructions](https://docs.roocode.com/features/custom-instructions)\n  - [Diagnostics Integration](https://docs.roocode.com/features/diagnostics-integration)\n  - [Customizing Modes](https://docs.roocode.com/features/custom-modes)\n  - [Diff/Fast Edits](https://docs.roocode.com/features/fast-edits)\n  - [Enhance Prompt](https://docs.roocode.com/features/enhance-prompt)\n  - [Import/Export/Reset Settings](https://docs.roocode.com/features/settings-management)\n  - [Intelligent Context Condensing](https://docs.roocode.com/features/intelligent-context-condensing)\n  - [Keyboard Navigation](https://docs.roocode.com/features/keyboard-shortcuts)\n  - [Roo Code Marketplace](https://docs.roocode.com/features/marketplace)\n  - [Message Queueing](https://docs.roocode.com/features/message-queueing)\n  - [Model Temperature](https://docs.roocode.com/features/model-temperature)\n  - [Multi-File Reads](https://docs.roocode.com/features/concurrent-file-reads)\n  - [.rooignore](https://docs.roocode.com/features/rooignore)\n  - [Slash Commands](https://docs.roocode.com/features/slash-commands)\n  - [Suggested Responses](https://docs.roocode.com/features/suggested-responses)\n  - [Task Todo List](https://docs.roocode.com/features/task-todo-list)\n  - [Terminal Shell Integration](https://docs.roocode.com/features/shell-integration)\n  - [MCP](https://docs.roocode.com/providers/litellm#)\n\n  - [Experimental](https://docs.roocode.com/providers/litellm#)\n\n  - [Additional Features](https://docs.roocode.com/features/more-features)\n- [Advanced Usage](https://docs.roocode.com/providers/litellm#)\n\n- [Model Providers](https://docs.roocode.com/providers/litellm#)\n\n  - [Anthropic](https://docs.roocode.com/providers/anthropic)\n  - [Claude Code](https://docs.roocode.com/providers/claude-code)\n  - [AWS Bedrock](https://docs.roocode.com/providers/bedrock)\n  - [Cerebras](https://docs.roocode.com/providers/cerebras)\n  - [DeepInfra](https://docs.roocode.com/providers/deepinfra)\n  - [DeepSeek](https://docs.roocode.com/providers/deepseek)\n  - [Doubao](https://docs.roocode.com/providers/doubao)\n  - [Featherless AI](https://docs.roocode.com/providers/featherless)\n  - [Fireworks AI](https://docs.roocode.com/providers/fireworks)\n  - [Chutes AI](https://docs.roocode.com/providers/chutes)\n  - [Google Gemini](https://docs.roocode.com/providers/gemini)\n  - [Glama](https://docs.roocode.com/providers/glama)\n  - [Groq](https://docs.roocode.com/providers/groq)\n  - [Hugging Face](https://docs.roocode.com/providers/huggingface)\n  - [Human Relay](https://docs.roocode.com/providers/human-relay)\n  - [IO Intelligence Provider](https://docs.roocode.com/providers/io-intelligence)\n  - [LM Studio](https://docs.roocode.com/providers/lmstudio)\n  - [LiteLLM](https://docs.roocode.com/providers/litellm)\n  - [Mistral AI](https://docs.roocode.com/providers/mistral)\n  - [Ollama](https://docs.roocode.com/providers/ollama)\n  - [OpenAI](https://docs.roocode.com/providers/openai)\n  - [OpenAI Compatible](https://docs.roocode.com/providers/openai-compatible)\n  - [OpenRouter](https://docs.roocode.com/providers/openrouter)\n  - [Qwen Code CLI](https://docs.roocode.com/providers/qwen-code)\n  - [Requesty](https://docs.roocode.com/providers/requesty)\n  - [Roo Code Cloud](https://docs.roocode.com/providers/roo-code-cloud)\n  - [SambaNova](https://docs.roocode.com/providers/sambanova)\n  - [Unbound](https://docs.roocode.com/providers/unbound)\n  - [Vercel AI Gateway](https://docs.roocode.com/providers/vercel-ai-gateway)\n  - [GCP Vertex AI](https://docs.roocode.com/providers/vertex)\n  - [VS Code Language Model API](https://docs.roocode.com/providers/vscode-lm)\n  - [xAI (Grok)](https://docs.roocode.com/providers/xai)\n  - [Z AI](https://docs.roocode.com/providers/zai)\n- [FAQ](https://docs.roocode.com/providers/litellm#)\n\n- [Contributing (GitHub)](https://github.com/RooCodeInc/Roo-Code/blob/main/CONTRIBUTING.md)\n- [Roocabulary (GitHub)](https://github.com/cannuri/Roocabulary)\n- [Update Notes](https://docs.roocode.com/providers/litellm#)\n\n\n- [Home page](https://docs.roocode.com/)\n- Model Providers\n- LiteLLM\n\nCopy Page\n\nOn this page\n\n# Using LiteLLM With Roo Code\n\nLiteLLM is a versatile tool that provides a unified interface to over 100 Large Language Models (LLMs) by offering an OpenAI-compatible API. This allows you to run a local server that can proxy requests to various model providers or serve local models, all accessible through a consistent API endpoint.\n\n**Website:** [https://litellm.ai/](https://litellm.ai/) (Main project) & [https://docs.litellm.ai/](https://docs.litellm.ai/) (Documentation)\n\n* * *\n\n## Key Benefits [​](https://docs.roocode.com/providers/litellm\\#key-benefits \"Direct link to Key Benefits\")\n\n- **Unified API:** Access a wide range of LLMs (from OpenAI, Anthropic, Cohere, HuggingFace, etc.) through a single, OpenAI-compatible API.\n- **Local Deployment:** Run your own LiteLLM server locally, giving you more control over model access and potentially reducing latency.\n- **Simplified Configuration:** Manage credentials and model configurations in one place (your LiteLLM server) and let Roo Code connect to it.\n- **Cost Management:** LiteLLM offers features for tracking costs across different models and providers.\n\n* * *\n\n## Setting Up Your LiteLLM Server [​](https://docs.roocode.com/providers/litellm\\#setting-up-your-litellm-server \"Direct link to Setting Up Your LiteLLM Server\")\n\nTo use LiteLLM with Roo Code, you first need to set up and run a LiteLLM server.\n\n### Installation [​](https://docs.roocode.com/providers/litellm\\#installation \"Direct link to Installation\")\n\n1. Install LiteLLM with proxy support:\n\n\n\n\n```codeBlockLines_e6Vv\npip install 'litellm[proxy]'\n\n```\n\n\n### Configuration [​](https://docs.roocode.com/providers/litellm\\#configuration \"Direct link to Configuration\")\n\n2. Create a configuration file ( `config.yaml`) to define your models and providers:\n\n\n\n\n```codeBlockLines_e6Vv\nmodel_list:\n     # Configure Anthropic models\n  - model_name: claude-3-7-sonnet\n    litellm_params:\n      model: anthropic/claude-3-7-sonnet-20250219\n      api_key: os.environ/ANTHROPIC_API_KEY\n\n# Configure OpenAI models\n  - model_name: gpt-4o\n    litellm_params:\n      model: openai/gpt-4o\n      api_key: os.environ/OPENAI_API_KEY\n\n# Configure Azure OpenAI\n  - model_name: azure-gpt-4\n    litellm_params:\n      model: azure/my-deployment-name\n      api_base: https://your-resource.openai.azure.com/\n      api_version: \"2023-05-15\"\n      api_key: os.environ/AZURE_API_KEY\n\n```\n\n### Starting the Server [​](https://docs.roocode.com/providers/litellm\\#starting-the-server \"Direct link to Starting the Server\")\n\n3. Start the LiteLLM proxy server:\n\n\n\n\n\n```codeBlockLines_e6Vv\n# Using configuration file (recommended)\nlitellm --config config.yaml\n\n# Or quick start with a single model\nexport ANTHROPIC_API_KEY=your-anthropic-key\nlitellm --model claude-3-7-sonnet-20250219\n\n```\n\n4. The proxy will run at `http://0.0.0.0:4000` by default (accessible as `http://localhost:4000`).\n   - You can also configure an API key for your LiteLLM server itself for added security.\n\nRefer to the [LiteLLM documentation](https://docs.litellm.ai/docs/) for detailed instructions on advanced server configuration and features.\n\n* * *\n\n## Configuration in Roo Code [​](https://docs.roocode.com/providers/litellm\\#configuration-in-roo-code \"Direct link to Configuration in Roo Code\")\n\nOnce your LiteLLM server is running, you have two options for configuring it in Roo Code:\n\n### Option 1: Using the LiteLLM Provider (Recommended) [​](https://docs.roocode.com/providers/litellm\\#option-1-using-the-litellm-provider-recommended \"Direct link to Option 1: Using the LiteLLM Provider (Recommended)\")\n\n1. **Open Roo Code Settings:** Click the gear icon () in the Roo Code panel.\n2. **Select Provider:** Choose \"LiteLLM\" from the \"API Provider\" dropdown.\n3. **Enter Base URL:**\n   - Input the URL of your LiteLLM server.\n   - Defaults to `http://localhost:4000` if left blank.\n4. **Enter API Key (Optional):**\n   - If you've configured an API key for your LiteLLM server, enter it here.\n   - If your LiteLLM server doesn't require an API key, Roo Code will use a default dummy key ( `\"dummy-key\"`), which should work fine.\n5. **Select Model:**\n   - Roo Code will attempt to fetch the list of available models from your LiteLLM server by querying the `${baseUrl}/v1/model/info` endpoint.\n   - The models displayed in the dropdown are sourced from this endpoint.\n   - Use the refresh button to update the model list if you've added new models to your LiteLLM server.\n   - If no model is selected, Roo Code defaults to `anthropic/claude-3-7-sonnet-20250219` (this is `litellmDefaultModelId`). Ensure this model (or your desired default) is configured and available on your LiteLLM server.\n\n### Option 2: Using OpenAI Compatible Provider [​](https://docs.roocode.com/providers/litellm\\#option-2-using-openai-compatible-provider \"Direct link to Option 2: Using OpenAI Compatible Provider\")\n\nAlternatively, you can configure LiteLLM using the \"OpenAI Compatible\" provider:\n\n1. **Open Roo Code Settings:** Click the gear icon () in the Roo Code panel.\n2. **Select Provider:** Choose \"OpenAI Compatible\" from the \"API Provider\" dropdown.\n3. **Enter Base URL:** Input your LiteLLM proxy URL (e.g., `http://localhost:4000`).\n4. **Enter API Key:** Use any string as the API key (e.g., `\"sk-1234\"`) since LiteLLM handles the actual provider authentication.\n5. **Select Model:** Choose the model name you configured in your `config.yaml` file.\n\n![Roo Code LiteLLM Provider Settings](https://docs.roocode.com/img/litellm/litellm.png)\n\n* * *\n\n## How Roo Code Fetches and Interprets Model Information [​](https://docs.roocode.com/providers/litellm\\#how-roo-code-fetches-and-interprets-model-information \"Direct link to How Roo Code Fetches and Interprets Model Information\")\n\nWhen you configure the LiteLLM provider, Roo Code interacts with your LiteLLM server to get details about the available models:\n\n- **Model Discovery:** Roo Code makes a GET request to `${baseUrl}/v1/model/info` on your LiteLLM server. If an API key is provided in Roo Code's settings, it's included in the `Authorization: Bearer ${apiKey}` header.\n- **Model Properties:** For each model reported by your LiteLLM server, Roo Code extracts and interprets the following:\n  - `model_name`: The identifier for the model.\n  - `maxTokens`: Maximum output tokens. Defaults to `8192` if not specified by LiteLLM.\n  - `contextWindow`: Maximum context tokens. Defaults to `200000` if not specified by LiteLLM.\n  - `supportsImages`: Determined from `model_info.supports_vision` provided by LiteLLM.\n  - `supportsPromptCache`: Determined from `model_info.supports_prompt_caching` provided by LiteLLM.\n  - `inputPrice` / `outputPrice`: Calculated from `model_info.input_cost_per_token` and `model_info.output_cost_per_token` from LiteLLM.\n  - `supportsComputerUse`: This flag is set to `true` if the underlying model identifier (from `litellm_params.model`, e.g., `openrouter/anthropic/claude-3.5-sonnet`) matches one of the Anthropic models predefined in Roo Code as suitable for \"computer use\" (see `COMPUTER_USE_MODELS` in technical details).\n\nRoo Code uses default values for some of these properties if they are not explicitly provided by your LiteLLM server's `/model/info` endpoint for a given model. The defaults are:\n\n- `maxTokens`: 8192\n- `contextWindow`: 200,000\n- `supportsImages`: `true`\n- `supportsComputerUse`: `true` (for the default model ID)\n- `supportsPromptCache`: `true`\n- `inputPrice`: 3.0 (µUSD per 1k tokens)\n- `outputPrice`: 15.0 (µUSD per 1k tokens)\n\n* * *\n\n## Tips and Notes [​](https://docs.roocode.com/providers/litellm\\#tips-and-notes \"Direct link to Tips and Notes\")\n\n- **LiteLLM Server is Key:** The primary configuration for models, API keys for downstream providers (like OpenAI, Anthropic), and other advanced features are managed on your LiteLLM server. Roo Code acts as a client to this server.\n- **Configuration Options:** You can use either the dedicated \"LiteLLM\" provider (recommended) for automatic model discovery, or the \"OpenAI Compatible\" provider for simple manual configuration.\n- **Model Availability:** The models available in Roo Code's \"Model\" dropdown depend entirely on what your LiteLLM server exposes through its `/v1/model/info` endpoint.\n- **Network Accessibility:** Ensure your LiteLLM server is running and accessible from the machine where VS Code and Roo Code are running (e.g., check firewall rules if not on `localhost`).\n- **Troubleshooting:** If models aren't appearing or requests fail:\n  - Verify your LiteLLM server is running and configured correctly.\n  - Check the LiteLLM server logs for errors.\n  - Ensure the Base URL in Roo Code settings matches your LiteLLM server's address.\n  - Confirm any API key required by your LiteLLM server is correctly entered in Roo Code.\n- **Computer Use Models:** The `supportsComputerUse` flag in Roo Code is primarily relevant for certain Anthropic models known to perform well with tool-use and function-calling tasks. If you are routing other models through LiteLLM, this flag might not be automatically set unless the underlying model ID matches the specific Anthropic ones Roo Code recognizes.\n\nBy leveraging LiteLLM, you can significantly expand the range of models accessible to Roo Code while centralizing their management.\n\n[Edit this page](https://github.com/RooCodeInc/Roo-Code-Docs/edit/main/docs/providers/litellm.md)\n\nLast updated on **Oct 3, 2025**\n\n[Previous\\\\\n\\\\\nLM Studio](https://docs.roocode.com/providers/lmstudio) [Next\\\\\n\\\\\nMistral AI](https://docs.roocode.com/providers/mistral)\n\n- [Key Benefits](https://docs.roocode.com/providers/litellm#key-benefits)\n- [Setting Up Your LiteLLM Server](https://docs.roocode.com/providers/litellm#setting-up-your-litellm-server)\n  - [Installation](https://docs.roocode.com/providers/litellm#installation)\n  - [Configuration](https://docs.roocode.com/providers/litellm#configuration)\n  - [Starting the Server](https://docs.roocode.com/providers/litellm#starting-the-server)\n- [Configuration in Roo Code](https://docs.roocode.com/providers/litellm#configuration-in-roo-code)\n  - [Option 1: Using the LiteLLM Provider (Recommended)](https://docs.roocode.com/providers/litellm#option-1-using-the-litellm-provider-recommended)\n  - [Option 2: Using OpenAI Compatible Provider](https://docs.roocode.com/providers/litellm#option-2-using-openai-compatible-provider)\n- [How Roo Code Fetches and Interprets Model Information](https://docs.roocode.com/providers/litellm#how-roo-code-fetches-and-interprets-model-information)\n- [Tips and Notes](https://docs.roocode.com/providers/litellm#tips-and-notes)\n\n![Roo Code Logo](https://docs.roocode.com/img/roo-code-logo-dark.png)\n\nEmpowering developers to build better software faster with AI-powered tools and insights.\n\n[GitHub](https://github.com/RooCodeInc/Roo-Code)[Discord](https://discord.gg/roocode)[Reddit](https://www.reddit.com/r/RooCode/)[X (Twitter)](https://x.com/roo_code)[LinkedIn](https://www.linkedin.com/company/roo-code)[TikTok](https://www.tiktok.com/@roo.code)[Bluesky](https://bsky.app/profile/roocode.bsky.social)\n\nGitHub\n\n- [Issues](https://github.com/RooCodeInc/Roo-Code/issues)\n- [Feature Requests](https://github.com/RooCodeInc/Roo-Code/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)\n\nDownload\n\n- [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline)\n- [Open VSX Registry](https://open-vsx.org/extension/RooVeterinaryInc/roo-cline)\n\nCompany\n\n- [Contact](mailto:support@roocode.com)\n- [Careers](https://careers.roocode.com/)\n- [Website Privacy Policy](https://roocode.com/privacy)\n- [Extension Privacy Policy](https://github.com/RooCodeInc/Roo-Code/blob/main/PRIVACY.md)\n\nLike most of the internet, we use cookies. Are you OK with that?\n\nDeclineAccept",
  "metadata": {
    "description": "Access over 100 LLMs through LiteLLM's unified OpenAI-compatible API in Roo Code. Simplify multi-model management and reduce costs.",
    "viewport": "width=device-width, initial-scale=1.0",
    "og:type": "website",
    "title": "Using LiteLLM With Roo Code | Roo Code Documentation",
    "docsearch:docusaurus_tag": "docs-default-current",
    "keywords": "litellm,roo code,api provider,unified api,openai compatible,multi model,llm proxy,local deployment,cost management",
    "twitter:creator": "@roo_code",
    "ogUrl": "https://docs.roocode.com/providers/litellm",
    "ogTitle": "Using LiteLLM With Roo Code | Roo Code Documentation",
    "og:title": "Using LiteLLM With Roo Code | Roo Code Documentation",
    "favicon": "https://docs.roocode.com/img/favicon.ico",
    "ogLocale": "en_US",
    "og:locale": "en_US",
    "twitter:site": "@roo_code",
    "generator": "Docusaurus v3.9.2",
    "twitter:image": "https://docs.roocode.com/img/social-share.jpg",
    "language": "en",
    "docusaurus_tag": "docs-default-current",
    "docsearch:language": "en",
    "docsearch:version": "current",
    "og:image": "https://docs.roocode.com/img/social-share.jpg",
    "og:url": "https://docs.roocode.com/providers/litellm",
    "twitter:card": "summary_large_image",
    "og:description": "Access over 100 LLMs through LiteLLM's unified OpenAI-compatible API in Roo Code. Simplify multi-model management and reduce costs.",
    "ogDescription": "Access over 100 LLMs through LiteLLM's unified OpenAI-compatible API in Roo Code. Simplify multi-model management and reduce costs.",
    "docusaurus_locale": "en",
    "docusaurus_version": "current",
    "ogImage": "https://docs.roocode.com/img/social-share.jpg",
    "scrapeId": "8dfc9716-098b-4466-aedb-240419c68ec1",
    "sourceURL": "https://docs.roocode.com/providers/litellm",
    "url": "https://docs.roocode.com/providers/litellm",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic",
    "cacheState": "miss",
    "creditsUsed": 1
  }
}